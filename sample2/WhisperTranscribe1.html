<!--
======================================================================
OpenAI Whisper 音声認識デモ用 HTML ファイル
======================================================================

このファイルは、ブラウザ上で音声を録音し、OpenAI Whisper API を使って音声をテキストに変換するためのデモ用 HTML ページです。

主な機能:
- マイクから音声を録音
- 無音検出（3秒間無音が続くと自動停止）
- 録音データを OpenAI Whisper API に送信
- 音声認識結果を表示

使用する技術:
- Web Audio API を用いたリアルタイムの音量検出
- MediaRecorder API を用いた音声録音
- OpenAI Whisper API を用いた音声認識

※ OpenAI API の認証キーは、実際の運用では環境変数などに保管することを推奨します。
※ 本コードはデモ用であり、実際の運用にはセキュリティ対策が必要です。
======================================================================
-->

<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>OpenAI Whisper 音声認識デモ</title>
  <style>
    #output { margin-top:20px; padding:10px; border:1px solid #ccc; min-height:2em; }
  </style>
</head>
<body>
  <h1>OpenAI Whisper 音声認識デモ</h1>
  <button id="start">録音開始</button>
  <button id="stop" disabled>録音停止</button>
  <div id="output">ここに結果が表示されます</div>

  <script>
    // --- 設定 ---
    // OpenAI Whisper API の認証キー（ご自身のキーに置き換えてください）
    const OPENAI_API_KEY = 'your api key';
    // 音量がこのレベル未満を「無音」とみなす閾値
    const SILENCE_THRESHOLD = 0.01;
    // 無音が続いたとみなすまでの時間（ミリ秒）
    const SILENCE_DELAY = 3000;

    // --- DOM 要素 ---
    const btnStart = document.getElementById('start');
    const btnStop  = document.getElementById('stop');
    const output  = document.getElementById('output');

    // --- 録音／無音検出用変数 ---
    let mediaRecorder, audioChunks = [];
    let silenceTimer, audioContext, analyser, sourceNode, processor;

    // 録音開始ボタンクリック時の処理
    btnStart.addEventListener('click', async () => {
      // 録音中状態に設定
      btnStart.disabled = true;
      output.textContent = '録音中…（無音3秒で自動停止）';

      // マイクアクセスを許可し、ストリームを取得
      const stream = await navigator.mediaDevices.getUserMedia({audio: true});

      // AudioContext を初期化
      audioContext = new AudioContext();
      analyser     = audioContext.createAnalyser();
      sourceNode   = audioContext.createMediaStreamSource(stream);
      processor    = audioContext.createScriptProcessor(2048, 1, 1);

      // 音声処理の接続を設定
      sourceNode.connect(analyser);
      analyser.connect(processor);
      processor.connect(audioContext.destination);

      // オーディオ処理イベントを設定
      processor.onaudioprocess = detectSilence;

      // 無音検出関数
      function detectSilence(e) {
        const data = e.inputBuffer.getChannelData(0);
        let rms = 0;
        for (let i = 0; i < data.length; i++) rms += data[i] * data[i];
        rms = Math.sqrt(rms / data.length);

        // 音量が無音閾値未満の場合
        if (rms < SILENCE_THRESHOLD) {
          // タイマーが設定されていない場合、無音検出タイマーをセット
          if (!silenceTimer) {
            silenceTimer = setTimeout(() => {
              silenceTimer = null;
              stopRecording();
            }, SILENCE_DELAY);
          }
        } else {
          // 音が戻った場合、タイマーをクリア
          if (silenceTimer) {
            clearTimeout(silenceTimer);
            silenceTimer = null;
          }
        }
      }

      // 録音データの初期化
      audioChunks = [];
      // MediaRecorder を初期化
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = onRecordingStop;
      mediaRecorder.start();
      btnStop.disabled = false;
    });

    // 録音停止ボタンクリック時の処理
    btnStop.addEventListener('click', stopRecording);

    // 録音を停止する関数
    function stopRecording() {
      // タイマーをクリア
      clearTimeout(silenceTimer);
      silenceTimer = null;

      // 録音中であれば停止処理を実行
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
        btnStop.disabled = true;
        // 音声処理の接続を切断
        processor.disconnect();
        sourceNode.disconnect();
        analyser.disconnect();
      }
    }

    // 録音停止後の処理（Whisper API に音声を送信）
    async function onRecordingStop() {
      output.textContent = '送信中…';
      // 録音データをBlobに変換
      const blob = new Blob(audioChunks, {type:'audio/webm'});
      const form = new FormData();
      form.append('file', blob, 'speech.webm');
      form.append('model', 'whisper-1');
      form.append('language', "ja")

      try {
        // Whisper API にPOSTリクエストを送信
        const res = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}` },
          body: form
        });
        const json = await res.json();
        // 結果を表示
        if (json.text) {
          output.textContent = json.text;
        } else {
          output.textContent = '認識に失敗しました';
        }
      } catch (err) {
        output.textContent = 'エラー: ' + err.message;
      } finally {
        // 録音ボタンを再有効化
        btnStart.disabled = false;
      }
    }
  </script>
</body>
</html>